{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/cost-aware-bo/t5_fine_tuning\n",
      "/home/jovyan/cost-aware-bo/t5_fine_tuning\n",
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"/home/jovyan/cost-aware-bo/t5_fine_tuning\")\n",
    "print(os.getcwd())\n",
    "!git pull\n",
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: conda [-h] [-v] [--no-plugins] [-V] COMMAND ...\n",
      "conda: error: argument COMMAND: invalid choice: 'activate' (choose from 'clean', 'compare', 'config', 'create', 'info', 'init', 'install', 'list', 'notices', 'package', 'remove', 'uninstall', 'rename', 'run', 'search', 'update', 'upgrade', 'doctor', 'env')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bash\n",
    "conda activate t5env\n",
    "cd /home/jovyan/cost-aware-bo/t5_fine_tuning\n",
    "git pull\n",
    "bash run_multi.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://10.127.79.238:32690/user/ridwan/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "ps -e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0__batch_size': 19, '1__learning_rate': 0.00035, '1__weight_decay': 0.584, '2__learning_rate': 0.001, '2__temperature': 1, '2__weight_decay': 0.329, '3__learning_rate': 0.001, '3__temperature': 1, '3__weight_decay': 0.329, '4__learning_rate': 0.001, '4__temperature': 1, '4__weight_decay': 0.329}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1447.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 fine tuning completed\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of distillation completed\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of distillation completed\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/jovyan/t5env/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 of distillation completed\n",
      "{'obj': 0.02519576630989427, 'costs': [17.747031927108765, 114.7081663608551, 283.6825838088989]}\n",
      "Total duration: 416.13929891586304\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Dict, Tuple\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils import (\n",
    "    tuning,\n",
    "    distillation,\n",
    "    load_hyperparameters,\n",
    "    inference,\n",
    "    select_first_n_stages,\n",
    ")\n",
    "\n",
    "from cachestore import Cache, LocalStorage\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--exp-name\",\n",
    "    type=str,\n",
    "    help=\"Specifies a unique experiment name\",\n",
    "    default=\"test-run3\",\n",
    ")\n",
    "parser.add_argument(\"--trial\", type=int, help=\"The trial number\", default=1)\n",
    "parser.add_argument(\n",
    "    \"--cache-root\", type=Path, default=\".cachestore\", help=\"Cache directory\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--acqf\",\n",
    "    type=str,\n",
    "    help=\"Acquisition function\",\n",
    "    choices=[\"EEIPU\", \"MS_CArBO\", \"EIPS\", \"CArBO\", \"EI\", \"RAND\"],\n",
    "    default=\"EI\",\n",
    ")\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "disable_cache = args.acqf != \"EEIPU\"\n",
    "cache = Cache(\n",
    "    f\"stacking_{args.exp_name}_{args.trial}_cache\",\n",
    "    storage=LocalStorage(args.cache_root),\n",
    "    disable=disable_cache,\n",
    ")\n",
    "\n",
    "num_samples = 2500  # 9540 seconds to finish 25K samples on 4 gpus\n",
    "\n",
    "\n",
    "# @task(cache=True, cache_key_file=\"hparams\", timer=True)\n",
    "@cache(ignore={\"output_dir\", \"dataset\"})\n",
    "def data_preprocessing(\n",
    "    dataset: Union[Path, str], output_dir: Union[Path, str], *, hparams\n",
    "):\n",
    "    \"\"\"Data Preprocessing Stage.\n",
    "\n",
    "    dataset (Path or str): path where the training dataset and the hyperparameters dataset are stored.\n",
    "    output_dir (Path or str): path where output results are stored. Must be unique for each run\n",
    "    \"\"\"\n",
    "    dataset = Path(dataset)\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    # start_time = time.time()\n",
    "    train_inputs_encodings, train_summaries_encodings = torch.load(\n",
    "        dataset / \"tokenized_train_data.pt\"\n",
    "    )\n",
    "    val_inputs_encodings, val_summaries_encodings = torch.load(\n",
    "        dataset / \"tokenized_validation_data.pt\"\n",
    "    )\n",
    "\n",
    "    if num_samples > 0:\n",
    "        train_inputs_encodings = {\n",
    "            key: value[:num_samples] for key, value in train_inputs_encodings.items()\n",
    "        }\n",
    "        train_summaries_encodings = {\n",
    "            key: value[:num_samples] for key, value in train_summaries_encodings.items()\n",
    "        }\n",
    "        val_inputs_encodings = {\n",
    "            key: value[:num_samples] for key, value in val_inputs_encodings.items()\n",
    "        }\n",
    "        val_summaries_encodings = {\n",
    "            key: value[:num_samples] for key, value in val_summaries_encodings.items()\n",
    "        }\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        train_inputs_encodings[\"input_ids\"],\n",
    "        train_inputs_encodings[\"attention_mask\"],\n",
    "        train_summaries_encodings[\"input_ids\"],\n",
    "        train_summaries_encodings[\"attention_mask\"],\n",
    "    )\n",
    "\n",
    "    val_dataset = TensorDataset(\n",
    "        val_inputs_encodings[\"input_ids\"],\n",
    "        val_inputs_encodings[\"attention_mask\"],\n",
    "        val_summaries_encodings[\"input_ids\"],\n",
    "        val_summaries_encodings[\"attention_mask\"],\n",
    "    )\n",
    "\n",
    "    batch_size = hparams[\"0__batch_size\"] * torch.cuda.device_count()\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    original_data = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    test_dataset = original_data[\"test\"]\n",
    "    test_dataset = test_dataset.shuffle(seed=42).select([i for i in range(10)])\n",
    "\n",
    "    # Write the data in the output directory.\n",
    "    test_dataset.save_to_disk(output_dir / \"testing_data\")\n",
    "    torch.save(train_dataloader, output_dir / \"training_data\")\n",
    "    torch.save(val_dataloader, output_dir / \"validation_data\")\n",
    "    with (output_dir / \"data_preprocessing.txt\").open(\"w\") as f:\n",
    "        f.write(\"Data preprocessing has been completed!\")\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # metrics = {\"cost\": end_time - start_time}\n",
    "    # with open(output_dir / \"metrics.json\", \"w\") as metrics_file:\n",
    "    #     json.dump(metrics, metrics_file)\n",
    "\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "# @task(cache=True, cache_key_file=\"hparams\", timer=True)\n",
    "@cache(ignore={\"output_dir\", \"dataset\", \"data_prepoc_output_path\", \"epochs\"})\n",
    "def fine_tuning(\n",
    "    dataset: Union[Path, str],\n",
    "    data_prepoc_output_path: Union[Path, str],\n",
    "    output_dir: Union[Path, str],\n",
    "    epochs,\n",
    "    *,\n",
    "    hparams,\n",
    "    global_epochs,\n",
    "    model_name: Union[Path, str],\n",
    ") -> Path:\n",
    "    \"\"\"Fine Tuning Stage.\"\"\"\n",
    "    dataset = Path(dataset)\n",
    "    output_dir = Path(output_dir)\n",
    "    data_prepoc_output_path = Path(data_prepoc_output_path)\n",
    "\n",
    "    # start_time = time.time()\n",
    "    model_path = tokenizer_path = model_name\n",
    "    if isinstance(model_name, Path) and model_name.exists():\n",
    "        model_path = model_path / \"fine_tuned_model\"\n",
    "        tokenizer_path = tokenizer_path / \"fine_tuned_tokenizer\"\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(tokenizer_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_path).to(\"cuda:0\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hparams[\"1__learning_rate\"],\n",
    "        weight_decay=hparams[\"1__weight_decay\"],\n",
    "    )\n",
    "\n",
    "    train_dataloader = torch.load(data_prepoc_output_path / \"training_data\")\n",
    "    val_dataloader = torch.load(data_prepoc_output_path / \"validation_data\")\n",
    "    validation_dataset = datasets.load_from_disk(dataset / \"validation_data\")\n",
    "    if num_samples > 0 and num_samples < 13368:\n",
    "        validation_dataset = validation_dataset.select(range(num_samples))\n",
    "    metrics, fine_tuned_model, fine_tuned_tokenizer = tuning(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        # num_epochs__1,\n",
    "        # hparams[\"1__num_epochs\"],\n",
    "        epochs,\n",
    "        tokenizer,\n",
    "        validation_dataset,\n",
    "    )\n",
    "\n",
    "    model_chkpt_path = output_dir / str(global_epochs)\n",
    "    fine_tuned_model.save_pretrained(model_chkpt_path / \"fine_tuned_model/\")\n",
    "    fine_tuned_tokenizer.save_pretrained(model_chkpt_path / \"fine_tuned_tokenizer/\")\n",
    "\n",
    "    # metrics[\"epoch_num\"].append(hparams[\"1__num_epochs\"])\n",
    "    metrics[\"learning_rate\"].append(hparams[\"1__learning_rate\"])\n",
    "    metrics[\"batch_size\"].append(hparams[\"0__batch_size\"])\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # metrics[\"cost\"] = end_time - start_time\n",
    "\n",
    "    # with open(output_dir / \"metrics.json\", \"w\") as metrics_file:\n",
    "    #     json.dump(metrics, metrics_file)\n",
    "\n",
    "    print(f\"Epoch {global_epochs} fine tuning completed\")\n",
    "\n",
    "    return model_chkpt_path\n",
    "\n",
    "\n",
    "# @task(cache=True, cache_key_file=\"hparams\")\n",
    "@cache(\n",
    "    ignore={\n",
    "        \"output_dir\",\n",
    "        \"dataset\",\n",
    "        \"fine_tuned_model_path\",\n",
    "        \"data_prepoc_output_path\",\n",
    "        \"epochs\",\n",
    "    }\n",
    ")\n",
    "def model_distillation(\n",
    "    dataset: Union[Path, str],\n",
    "    data_prepoc_output_path: Union[Path, str],\n",
    "    fine_tuned_model_path: Union[Path, str],\n",
    "    output_dir: Union[Path, str],\n",
    "    epochs,\n",
    "    *,\n",
    "    hparams,\n",
    "    student_model_name: Union[Path, str],\n",
    "    global_epochs,\n",
    ") -> Tuple[float, Path]:\n",
    "    \"\"\"Distillation Stage.\"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # start_time = time.time()\n",
    "    # model_name = \"t5-small\"\n",
    "\n",
    "    # Load your fine-tuned t5-small teacher model and tokenizer\n",
    "    teacher_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        fine_tuned_model_path / \"fine_tuned_model/\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    student_model_path = student_tokenizer_path = student_model_name\n",
    "    if isinstance(student_model_name, Path) and student_model_name.exists():\n",
    "        student_model_path = student_model_path / \"distilled_model\"\n",
    "        student_tokenizer_path = student_tokenizer_path / \"distilled_tokenizer\"\n",
    "\n",
    "    # Load T5-tiny student model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\n",
    "        student_tokenizer_path, d_model=128, d_ff=512, d_kv=64, num_layers=2\n",
    "    )\n",
    "    student_config = T5Config.from_pretrained(\n",
    "        student_model_path, d_model=128, d_ff=512, d_kv=64, num_layers=2\n",
    "    )\n",
    "    student_model = T5ForConditionalGeneration(student_config).to(\"cuda\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        student_model.parameters(),\n",
    "        lr=hparams[\"2__learning_rate\"],\n",
    "        weight_decay=hparams[\"2__weight_decay\"],\n",
    "    )\n",
    "\n",
    "    # Define your training & validation dataset and dataloader\n",
    "    train_dataloader = torch.load(data_prepoc_output_path / \"training_data\")\n",
    "    val_dataloader = torch.load(data_prepoc_output_path / \"validation_data\")\n",
    "    validation_dataset = datasets.load_from_disk(dataset / \"validation_data\")\n",
    "\n",
    "    if num_samples > 0 and num_samples < 13368:\n",
    "        validation_dataset = validation_dataset.select(range(num_samples))\n",
    "\n",
    "    metrics, distilled_model, distilled_tokenizer = distillation(\n",
    "        teacher_model,\n",
    "        student_model,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        optimizer,\n",
    "        # num_epochs__2,\n",
    "        # hparams[\"2__num_epochs\"],\n",
    "        epochs,\n",
    "        tokenizer,\n",
    "        validation_dataset,\n",
    "        hparams[\"2__temperature\"],\n",
    "        output_dir,\n",
    "    )\n",
    "\n",
    "    model_chkpt_path = output_dir / str(global_epochs)\n",
    "    distilled_model.save_pretrained(model_chkpt_path / \"distilled_model\")\n",
    "    distilled_tokenizer.save_pretrained(model_chkpt_path / \"distilled_tokenizer\")\n",
    "\n",
    "    # metrics[\"epoch_num\"].append(hparams[\"2__num_epochs\"])\n",
    "    metrics[\"learning_rate\"].append(hparams[\"2__learning_rate\"])\n",
    "    metrics[\"batch_size\"].append(hparams[\"0__batch_size\"])\n",
    "    # end_time = time.time()\n",
    "    # metrics[\"cost\"] = end_time - start_time\n",
    "    # with open(output_dir / \"metrics.json\", \"w\") as metrics_file:\n",
    "    #     json.dump(metrics, metrics_file)\n",
    "    \n",
    "    print(f\"Epoch {global_epochs} of distillation completed\")\n",
    "\n",
    "\n",
    "    return metrics[\"rouge_scores\"][0][\"rougeLsum\"], model_chkpt_path\n",
    "\n",
    "\n",
    "# @task(cache=True, cache_key_file=\"hparams\")\n",
    "def model_inference(\n",
    "    data_prepoc_output_path: Union[Path, str],\n",
    "    fine_tuned_model_path: Union[Path, str],\n",
    "    # dataset: Union[Path, str],\n",
    "    distilled_model_path: Union[Path, str],\n",
    "    output_dir: Union[Path, str],\n",
    "    *,\n",
    "    hparams,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    fine_tuned_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        fine_tuned_model_path / \"fine_tuned_model/\"\n",
    "    )\n",
    "    fine_tuned_tokenizer = T5Tokenizer.from_pretrained(\n",
    "        fine_tuned_model_path / \"fine_tuned_tokenizer/\"\n",
    "    )\n",
    "    # Load the distilled model\n",
    "    distilled_model = T5ForConditionalGeneration.from_pretrained(\n",
    "        distilled_model_path / \"distilled_model/\"\n",
    "    )\n",
    "    # Quantize the distilled model\n",
    "    quantized_distilled_model = torch.quantization.quantize_dynamic(\n",
    "        distilled_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    quantized_distilled_tokenizer = T5Tokenizer.from_pretrained(\n",
    "        distilled_model_path / \"distilled_tokenizer/\"\n",
    "    )\n",
    "    test_dataset = datasets.load_from_disk(data_prepoc_output_path / \"testing_data\")\n",
    "    test_data = test_dataset[\"article\"]\n",
    "    reference_summaries = test_dataset[\"highlights\"]\n",
    "\n",
    "    # Inference for fine-tuned model\n",
    "    fine_tuned_metrics = inference(\n",
    "        fine_tuned_model, fine_tuned_tokenizer, test_data, reference_summaries\n",
    "    )\n",
    "\n",
    "    # Inference for quantized model\n",
    "    quantized_metrics = inference(\n",
    "        quantized_distilled_model,\n",
    "        quantized_distilled_tokenizer,\n",
    "        test_data,\n",
    "        reference_summaries,\n",
    "    )\n",
    "\n",
    "    metrics = {\n",
    "        \"fine_tuned_metrics\": fine_tuned_metrics,\n",
    "        \"quantized_metrics\": quantized_metrics,\n",
    "        \"cost\": 0,\n",
    "    }\n",
    "    end_time = time.time()\n",
    "    metrics[\"cost\"] = end_time - start_time\n",
    "\n",
    "    with open(output_dir / \"metrics.json\", \"w\") as metrics_file:\n",
    "        json.dump(metrics, metrics_file)\n",
    "\n",
    "    print(\"Inference has been completed\")\n",
    "\n",
    "    return output_dir\n",
    "\n",
    "\n",
    "# @task(cache=True, cache_key_file=\"hparams\")\n",
    "def generate_output(\n",
    "    data_prepoc_output_path: Union[Path, str],\n",
    "    fine_tuned_model_path: Union[Path, str],\n",
    "    distilled_model_path: Union[Path, str],\n",
    "):\n",
    "    \"\"\"Output Generation.\"\"\"\n",
    "    # with open(dataset / \"initial_hparams.json\") as initial_hparams:\n",
    "    #     initial_hps = json.load(initial_hparams)\n",
    "\n",
    "    final_metrics = {}\n",
    "\n",
    "    # with open(dataset / \"hparams.json\") as _hparams:\n",
    "    #     hyperparmeters = json.load(_hparams)\n",
    "\n",
    "    # hp_dataset = hyperparmeters[\"dataset\"]\n",
    "\n",
    "    with open(data_prepoc_output_path / \"metrics.json\") as _metrics:\n",
    "        data_metrics = json.load(_metrics)\n",
    "\n",
    "    with open(fine_tuned_model_path / \"metrics.json\") as _metrics:\n",
    "        fine_tuning_metrics = json.load(_metrics)\n",
    "\n",
    "    with open(distilled_model_path / \"metrics.json\") as _metrics:\n",
    "        distillation_metrics = json.load(_metrics)\n",
    "\n",
    "    # with open(inference_output / \"metrics.json\") as _metrics:\n",
    "    #     inference_metrics = json.load(_metrics)\n",
    "\n",
    "    final_metrics[\"cost\"] = [\n",
    "        data_metrics[\"cost\"],\n",
    "        fine_tuning_metrics[\"cost\"],\n",
    "        distillation_metrics[\"cost\"],\n",
    "        # inference_metrics[\"cost\"],\n",
    "    ]\n",
    "\n",
    "    final_metrics[\"obj\"] = distillation_metrics[\"rouge_scores\"][0][\"rougeLsum\"]\n",
    "\n",
    "    # obj_validation_loss = metrics[\"validation_loss\"]\n",
    "    # obj_validation_loss_avg = sum(obj_validation_loss) / len(obj_validation_loss)\n",
    "\n",
    "    # hp_dataset[\"obj\"].append(obj_validation_loss_avg)\n",
    "\n",
    "    # initial_hps[\"dataset\"] = hp_dataset\n",
    "\n",
    "    # with (output_dir / \"hparams_file.json\").open(\"w\") as f:\n",
    "    #     json.dump(final_metrics, f)\n",
    "\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "def t5_fine_tuning(\n",
    "    dataset: Union[Path, str],\n",
    "    output_dir: Union[Path, str],\n",
    "    stg_hparams: List[Dict],\n",
    "    ft_num_epochs: int,\n",
    "    fine_tune_num_stgs: int,\n",
    "    dstl_num_epochs: int,\n",
    "    dstl_num_stgs: int,\n",
    "):\n",
    "    \"\"\"Main Pipeline.\"\"\"\n",
    "    tot_num_stgs = 1 + fine_tune_num_stgs + dstl_num_stgs\n",
    "    tot_num_hp_stgs = len(set(int(key.split(\"__\")[0]) for key in stg_hparams))\n",
    "    assert tot_num_stgs == tot_num_hp_stgs, f\"The total number of stages in the pipeline, {tot_num_stgs}, is not equal to the number hyperparameter stages {tot_num_hp_stgs}\"\n",
    "    \n",
    "    curr_stg = 1\n",
    "    data_preproc_hp = select_first_n_stages(stg_hparams, curr_stg)\n",
    "    tuning_hps = [select_first_n_stages(stg_hparams, stg+1+curr_stg) for stg in range(fine_tune_num_stgs)]\n",
    "    curr_stg += fine_tune_num_stgs\n",
    "    distillation_hps = [select_first_n_stages(stg_hparams, stg+1+curr_stg) for stg in range(dstl_num_stgs)]\n",
    "\n",
    "    start_data_proc = time.time()\n",
    "    data_prepoc_output_path = data_preprocessing(\n",
    "        dataset, output_dir / \"data_preprocessing\", hparams=data_preproc_hp\n",
    "    )\n",
    "\n",
    "    start_fine_tune = time.time()\n",
    "    fine_tuned_model_path = \"t5-small\"\n",
    "    global_epochs = 0\n",
    "    ft_epochs_per_stage = (ft_num_epochs // fine_tune_num_stgs) + (1 if (ft_num_epochs % fine_tune_num_stgs) else 0)\n",
    "    while global_epochs < ft_num_epochs:\n",
    "        epochs = min(ft_epochs_per_stage, ft_num_epochs - global_epochs)\n",
    "        print(epochs)\n",
    "        fine_tuned_model_path = fine_tuning(\n",
    "            dataset,\n",
    "            data_prepoc_output_path,\n",
    "            output_dir / \"fine_tuning\",\n",
    "            epochs,\n",
    "            hparams=tuning_hps.pop(0),\n",
    "            global_epochs=global_epochs,\n",
    "            model_name=fine_tuned_model_path,\n",
    "        )\n",
    "        global_epochs += ft_epochs_per_stage\n",
    "\n",
    "    start_distil = time.time()\n",
    "    distilled_model_path = \"t5-small\"\n",
    "    global_epochs = 0\n",
    "    dstl_epochs_per_stage = (dstl_num_epochs // dstl_num_stgs) + (1 if (dstl_num_epochs % dstl_num_stgs) else 0)\n",
    "    while global_epochs < dstl_num_epochs:\n",
    "        epochs = min(dstl_epochs_per_stage, dstl_num_epochs - global_epochs)\n",
    "        print(epochs)\n",
    "        rougeLsum, distilled_model_path = model_distillation(\n",
    "            dataset,\n",
    "            data_prepoc_output_path,\n",
    "            fine_tuned_model_path,\n",
    "            output_dir / \"model_distillation\",\n",
    "            epochs,\n",
    "            hparams=distillation_hps.pop(0),\n",
    "            student_model_name=distilled_model_path,\n",
    "            global_epochs=global_epochs,\n",
    "        )\n",
    "        global_epochs += dstl_epochs_per_stage\n",
    "    # inference_output = model_inference(\n",
    "    #     data_prepoc_output_path,\n",
    "    #     fine_tuned_model_path,\n",
    "    #     distilled_model_path,\n",
    "    #     output_dir,\n",
    "    #     hparams=stg_hparams,\n",
    "    # )\n",
    "\n",
    "    end = time.time()\n",
    "    # obj = generate_output(\n",
    "    #     data_prepoc_output_path, fine_tuned_model_path, distilled_model_path\n",
    "    # )\n",
    "    stage1_cost = start_fine_tune - start_data_proc\n",
    "    stage2_cost = start_distil - start_fine_tune\n",
    "    stage3_cost = end - start_distil\n",
    "\n",
    "    return {\"obj\": rougeLsum, \"costs\": [stage1_cost, stage2_cost, stage3_cost]}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    dataset = Path(\"inputs\")\n",
    "    output_dir = Path(\"outputs\") / time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n",
    "    stg_hparams = load_hyperparameters(dataset / \"hparams_multi.json\")\n",
    "    print(stg_hparams)\n",
    "    output = t5_fine_tuning(\n",
    "        dataset,\n",
    "        output_dir,\n",
    "        stg_hparams,\n",
    "        ft_num_epochs=2,\n",
    "        fine_tune_num_stgs=1,\n",
    "        dstl_num_epochs=6,\n",
    "        dstl_num_stgs=3,\n",
    "    )\n",
    "    print(output)\n",
    "    print(\"Total duration:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The total number of stages in the pipeline, 4, is not equal to the number hyperparameter stages 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mlocaltime())\n\u001b[1;32m      4\u001b[0m stg_hparams \u001b[38;5;241m=\u001b[39m load_hyperparameters(dataset \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhparams.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mt5_fine_tuning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstg_hparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mft_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfine_tune_num_stgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdstl_num_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdstl_num_stgs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal duration:\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[9], line 417\u001b[0m, in \u001b[0;36mt5_fine_tuning\u001b[0;34m(dataset, output_dir, stg_hparams, ft_num_epochs, fine_tune_num_stgs, dstl_num_epochs, dstl_num_stgs)\u001b[0m\n\u001b[1;32m    415\u001b[0m tot_num_stgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m fine_tune_num_stgs \u001b[38;5;241m+\u001b[39m dstl_num_stgs\n\u001b[1;32m    416\u001b[0m tot_num_hp_stgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mint\u001b[39m(key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m stg_hparams))\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tot_num_stgs \u001b[38;5;241m==\u001b[39m tot_num_hp_stgs, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total number of stages in the pipeline, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot_num_stgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is not equal to the number hyperparameter stages \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtot_num_hp_stgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m curr_stg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    420\u001b[0m data_preproc_hp \u001b[38;5;241m=\u001b[39m select_first_n_stages(stg_hparams, curr_stg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The total number of stages in the pipeline, 4, is not equal to the number hyperparameter stages 3"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset = Path(\"inputs\")\n",
    "output_dir = Path(\"outputs\") / time.strftime(\"%Y-%m-%d_%H:%M:%S\", time.localtime())\n",
    "stg_hparams = load_hyperparameters(dataset / \"hparams.json\")\n",
    "output = t5_fine_tuning(\n",
    "    dataset,\n",
    "    output_dir,\n",
    "    stg_hparams,\n",
    "    ft_num_epochs=2,\n",
    "    fine_tune_num_stgs=1,\n",
    "    dstl_num_epochs=6,\n",
    "    dstl_num_stgs=2,\n",
    ")\n",
    "print(output)\n",
    "print(\"Total duration:\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "26%3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "t5kernel",
   "language": "python",
   "name": "t5kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
